%-------------------------------------------------------------------------------
\section{Evaluation}
%-------------------------------------------------------------------------------
\subsection{Methodology}
In evaluation, we use a Dell r740 server with two Intel Xeon E5-2640V3 processors(2.6Ghz), 64GB DRAM, and Intel SSD of 480GB. For PM, we use Intel Optane DC 256 GB Persistent Memory Module, and for the operation system, we use Ubuntu 18.04LTS and Linux kernel version we use is 4.15.
We run PC-DB and RocksDB in this platform by contrast. In PC-DB, we use the APP Direct Mode of Intel Optane DC Persistent Memory Module. In RocksDB and our PC-DB, the MemTable size in RocksDB and PC-DB is set to 64MB, and the key size is set to a fixed size: 20 KB, and all SSTable files are stored in the SSD in both database. 
To evaluate the performance of the two database, we use the db\_bench benchmarks as microbenchmark and the YCSB as real world workload benchmarks.  

Generally our design can be divided into two parts: to get better consistency in exchange of performance, we use PM to replace DRAM; and to improve the read amplification in the validation phase in OCC, we use DRAM to store the outstanding transaction version. 
\subsection{Using a Persistent MemTable}
In order to better understand the effect of the two strategies, we first evaluate the performance of RocksDB and a modified version of RocksDB by replacing the DRAM using PM.  We design the experiment with the random write workload from db$\_$bench over various value sizes and the estimate the evaluation result for the write latency and the data written to the disk during the experiment. 

In the expected result of write latency, the write latency of RocksDB is slightly higher than that of RocksDB+PM. This is because 1)the write and read latency of PM is higher than the DRAM. 2) RocksDB have to flush WAL to the disk constantly, reducing the performance of RocksDB. However, RocksDB+PM can persistent the inserted data as soon as it is inserted to MemTable, and it can also provide stronger durability. As for the figure of total amount of write, RocksDB+PM can reduce the amount of write data by about $16\%$ compared to the result of RocksDB. This is because RocksDB+PM don`t have to write the WAL. 

For fault recovery, we first use  db\_bench random write workload to insert half the keys and then trigger system crash to simulate fault. We also perform experiment with different memtable size. In the expected results, the failure recovery speed of PC-DB is faster than RocksDB in all memtable size. This is because when the system recovers after a crash, PC-DB does not need to scan the Write-Ahead-Log. It can use the root pointer of skiplist stored in the MANIFEST file to remap NVM memtable for data recovery. As the memtable size increases, the recovery speed increases more significantly, because the larger the memtable is, the more Write-Ahead-Log need to be scanned.
\subsection{Result with Microbenchmarks}
We compare the operation throughputs of random write and random read workloads of PC-DB and RocksDB in this experiment, and estimate the expected result.
For the random write operations, PC-DB provides slightly higher throughput than the RocksDB over all the value sizes. As we have talked before, the write performance of RocksDB is almost the same as RocksDB+PM, and the introducing of DRAM has almost no effect of write performance, because we just need to insert or update the version of the records when a transaction is committed. This overhead of this operation is small. 
For the random operations, PC-DB also shows a similar performance as RocksDB. This because the read latency of PM is almost the same as that of DRAM, and if PC-DB can not get the key in PM, it will search the disk for the record, just like the RocksDB.

\subsection{Results with YCSB}
YCSB consists of six workloads: A, B, C, D, E. We first load workload A, and run workload A, B, C, D in order. Then we load workload E and run it. Note that in workload A, E, write operation has a more share in the total operations. We estimate their throughput, latency, and the data written to disk in this experiment.
In our expected evaluation result of the PC-DB and the RocksDB`s throughput and write latency , the throughput of PC-DB are higher than those of RocksDB and the latency of PC-DB is smaller than the RocksDB for all workloads, which is because we leverage DRAM as the cache. In the validation phase of OCC, we first search the PM for the latest version of a record, and if it is not found, we search it in the DRAM. While in RocksDB, we have to search the disk for the version if it is not found in the DRAM, which will cost a lot. We can also notice that if the write operations have more shares in the total operations, the PC-DB performs better.
In our expected evaluation result of their amount of data written to disk, the total amount of write in PC-DB is much smaller than that of RocksDB for all workloads. this is the same reason as the $5.2$: we don`t have to flush WAL in PC-DB.